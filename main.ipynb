{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "\n",
    "df = df.drop([\"id\"], axis=1)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Data prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation data to numerical values\n",
    "\n",
    "First of all, we need to transform every non-numerical data into numerical values. It will concern the columns *gender*, *ever_married*, *work_type*, *Residence_type*, *smoking_status*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"gender\"].value_counts())\n",
    "\n",
    "gender_type_map = {\"Female\": 2, \"Male\": 1, \"Other\": 0}\n",
    "df[\"gender\"] = df[\"gender\"].map(gender_type_map)\n",
    "\n",
    "print(df[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"work_type\"].value_counts())\n",
    "\n",
    "work_type_map = {\"Private\": 4, \"Self-employed\": 3, \"children\": 2, \"Govt_job\": 1, \"Never_worked\": 0}\n",
    "df[\"work_type\"] = df[\"work_type\"].map(work_type_map)\n",
    "\n",
    "print(df[\"work_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Residence_type\"].value_counts())\n",
    "\n",
    "residence_type_map = {\"Urban\":1, \"Rural\":0}\n",
    "df[\"Residence_type\"] = df[\"Residence_type\"].map(residence_type_map)\n",
    "\n",
    "print(df[\"Residence_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"ever_married\"].value_counts())\n",
    "\n",
    "married_type_map = {\"Yes\":1, \"No\":0}\n",
    "df[\"ever_married\"] = df[\"ever_married\"].map(married_type_map)\n",
    "\n",
    "print(df[\"ever_married\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"smoking_status\"].value_counts())\n",
    "\n",
    "smoking_type_map = {\"smokes\": 3, \"formerly smoked\": 2, \"never smoked\": 1, \"Unknown\": 0}\n",
    "df[\"smoking_status\"] = df[\"smoking_status\"].map(smoking_type_map)\n",
    "\n",
    "print(df[\"smoking_status\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping NaN values\n",
    "\n",
    "We saw ealier that the column *bmi* contains NaN values. So we will simply remove the rows containing this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"stroke\"].value_counts())\n",
    "print(df[\"stroke\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the dataset is imbalanced, with only 4% of of data representing a stroke. The risk of having an imbalanced dataset is that the model will probably overfit on the domimant class, here the class 0 (not a stroke).\n",
    "The model can still have a high accuracy but still be completely incorrect.\n",
    "Since the goal of our model is to predict if a stroke will happen, the most important metric to look at isn't the accuracy but the recall of the class 1. We want our model to predict the most stroke possible.\n",
    "\n",
    "If we try to use our current data in a logistic regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = copy.deepcopy(df)\n",
    "\n",
    "labels = df_copy[\"stroke\"]\n",
    "# features = df.drop([\"stroke\", \"Residence_type\", \"gender\"], axis=1)\n",
    "features = df_copy.drop(\"stroke\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter=400)\n",
    "\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, zero_division=1))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, zero_division=1))\n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, even tought the accuracy score is high, the model is completely false because it only predicted data to be of class 0, which means it detected 0 strokes. \n",
    "This is a critical error that we need to fix by balancing the data.\n",
    "\n",
    "Here, we will use up-sampling to even the number of features in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = df[df[\"stroke\"]==0]\n",
    "df_minority = df[df[\"stroke\"]==1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,\n",
    "                                 n_samples=4700,\n",
    "                                 random_state=123)\n",
    "\n",
    "df = pd.concat([df_majority, df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"stroke\"]\n",
    "# features = df.drop([\"stroke\", \"Residence_type\", \"gender\"], axis=1)\n",
    "features = df.drop(\"stroke\", axis=1)\n",
    "\n",
    "print(labels.value_counts())\n",
    "print(labels.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the date for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = df.corr()\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_pred):\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision: \", precision_score(y_test, y_pred, zero_division=1))\n",
    "    print(\"Recall: \", recall_score(y_test, y_pred, zero_division=1))\n",
    "    print(\"F1 Score: \", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(max_iter=400)\n",
    "\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "acc = logistic_regression.score(X_test, y_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors = 7)\n",
    "\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "acc = knn_classifier.score(X_test, y_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('healthcare-stroke-prediction-fu-Cbw5V-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4696ca6e2399baf76289359bbce7f3ba67aa9b0dc385bd7835b24212c371c0a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
